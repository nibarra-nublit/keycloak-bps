## 1. Conceptos clave de Observabilidad en Keycloak

La **observabilidad** es la capacidad de un sistema para exponer información que facilite monitorear su estado interno y diagnosticar problemas. En Keycloak, la observabilidad se apoya en varios pilares principales:

- **Métricas**: Valores numéricos que reflejan el desempeño y estado de Keycloak (por ejemplo, número de solicitudes, tiempos de respuesta, uso de memoria). Keycloak expone métricas en formato Prometheus para su recopilación periódica y análisis.
- **Trazas (tracing)**: Información detallada del recorrido de una petición a través de los componentes internos de Keycloak. Las trazas distribuidas permiten seguir una solicitud (ej. un login) a lo largo de diferentes etapas (desde la llegada HTTP, consultas a la base de datos, hasta llamadas a servicios externos), facilitando diagnosticar cuellos de botella y depurar errores.
- **Eventos**: Acciones o sucesos significativos en Keycloak, como inicios de sesión, cierres de sesión, creación de usuarios, errores de autenticación, etc. Keycloak genera eventos de usuario y de administración; tradicionalmente podían almacenarse en logs o base de datos para auditoría. En el contexto de observabilidad, _métricas de eventos_ convierten la ocurrencia de ciertos eventos (ej. logins exitosos o fallidos) en contadores métricos que se pueden monitorizar.
- **Salud (health)**: Indicadores binarios sobre si el servicio está funcionando correctamente. Keycloak proporciona endpoints de _health check_ para conocer su estado de vida, preparación y arranque. Estos endpoints devuelven un estado simple (`UP` o `DOWN`) que se puede integrar con orquestadores (como Kubernetes) o herramientas de monitoreo para detectar caídas o problemas críticos rápidamente.

## 2. Configuración de métricas y salud en Keycloak

Para aprovechar la observabilidad en Keycloak, primero debemos habilitar que exponga sus datos de métricas y salud. Estas características están deshabilitadas de forma predeterminada por razones de rendimiento y seguridad, por lo que debemos activarlas explícitamente.

**2.1 Habilitar las métricas**

Keycloak incluye soporte nativo para métricas con formato **Prometheus/OpenMetrics**. Para activarlas, se debe iniciar el servidor con la opción correspondiente. Esto se puede lograr de dos formas equivalentes:

- Vía línea de comando al iniciar Keycloak:
    
```bash
bin/kc.sh start --metrics-enabled=true
```
    
- Vía variable de entorno (útil en contenedores Docker):
    
```bash
`KC_METRICS_ENABLED=true`    
```
    

Una vez habilitadas, Keycloak expondrá un endpoint HTTP para las métricas en la interfaz de administración (por defecto en el puerto `9000`). El endpoint por defecto es: **`/metrics`**. Al acceder a esa URL, se obtendrá un texto con el formato de métricas de Prometheus (tipo OpenMetrics), que lista múltiples métricas con sus nombres, tipos, etiquetas (_labels_) y valores actuales. Por ejemplo, algunas líneas típicas pueden ser:

```text
# HELP jvm_memory_used_bytes The amount of memory currently used by the JVM
# TYPE jvm_memory_used_bytes gauge
jvm_memory_used_bytes{area="heap",} 1.2345678e7

# HELP keycloak_user_events_total Keycloak user events
# TYPE keycloak_user_events_total counter
keycloak_user_events_total{realm="master", event="login", error=""} 5.0
keycloak_user_events_total{realm="master", event="login", error="invalid_user_credentials"} 2.0
```

En el ejemplo anterior, se observan métricas de uso de memoria de la JVM y contadores de eventos de usuario (5 logins exitosos y 2 logins fallidos por credenciales inválidas en el realm "master"). Estas métricas estarán disponibles para que Prometheus u otra herramienta las _scrapee_ (recopile) periódicamente.

**Opciones relevantes de configuración de métricas**: Keycloak ofrece parámetros adicionales para personalizar la recopilación de métricas:

- `http-metrics-histograms-enabled`: Si se establece en `true`, habilita el registro de histogramas para la duración de las solicitudes HTTP. Por defecto es `false`. Los histogramas permiten calcular latencias en diferentes percentiles (p. ej. p90, p95) a costa de exponer más datos.
- `http-metrics-slos`: Permite definir _buckets_ personalizados para el histograma de latencia HTTP, normalmente usado para alinear con objetivos de nivel de servicio (SLOs). Se especifica una lista de tiempos en milisegundos separados por comas. Por ejemplo: `5,10,25,50,250,500,1000,2500,5000,10000` definirá buckets desde 5 ms hasta 10 s. Si se usa esta opción, puede combinarse con el histograma por defecto o sustituirlo.
- `cache-metrics-histograms-enabled`: Similar al anterior, pero para histogramas de métricas de caches internas (Infinispan). Deshabilitado por defecto.
- `metrics-enabled`: (Ya mencionado) habilita las métricas (valor por defecto `false`).

Todas estas opciones pueden configurarse mediante CLI (ej. `--http-metrics-histograms-enabled=true`) o variables de entorno equivalentes (ej. `KC_HTTP_METRICS_HISTOGRAMS_ENABLED=true`). En un entorno Docker, típicamente se usan las variables de entorno.

**2.2 Habilitar los checks de salud**

Keycloak expone endpoints HTTP especiales que reportan el **estado de salud** de la instancia. Existen cuatro endpoints relacionados:

- **`/health`** – Estado general de salud, agregando información de varios sub-checks.
- **`/health/ready`** – Indica si el servidor está listo para recibir tráfico (ej. que dependencias como la base de datos estén accesibles).
- **`/health/live`** – Indica si el servidor está vivo (en funcionamiento, aunque quizá aún inicializando).
- **`/health/started`** – Indica si el servidor completó su secuencia de arranque.

Estos endpoints devuelven HTTP 200 (OK) si el estado es saludable, o 503 (Service Unavailable) si algún check falla. El contenido es un JSON con un resumen del estado. Por ejemplo, una respuesta exitosa podría ser:

```json
{ "status": "UP", "checks": [] }
```

Y si incluimos el check de base de datos, el JSON podría listar un check específico, por ejemplo:

```json
{
  "status": "UP",
  "checks": [
    { "name": "Keycloak database connections health check", "status": "UP" }
  ]
}
```

De forma predeterminada, los checks están deshabilitados (no reportan nada). Para activarlos se usa la opción `health-enabled`. Igual que con las métricas, podemos habilitarlo al iniciar Keycloak con:

- CLI: `--health-enabled=true`
- Entorno: `KC_HEALTH_ENABLED=true`

Usualmente, conviene habilitar _ambos_ métricas y health al mismo tiempo para tener una visibilidad completa. Incluso, algunos checks de salud requieren las métricas habilitadas para funcionar (por ejemplo, el check de conexión a base de datos necesita que esté activo el subsistema de métricas)​. Es común ver la activación conjunta así:

```bash
bin/kc.sh start --health-enabled=true --metrics-enabled=true
```

Con esto, los endpoints `/health` estarán disponibles en el puerto de administración (9000 por defecto). La forma de usar estos endpoints es integrarlos con herramientas externas: por ejemplo, en **Kubernetes** se pueden configurar _probes_ HTTP que consulten `/health/ready` para determinar cuándo un pod de Keycloak está listo, y `/health/live` para reiniciarlo si deja de responder. En entornos tradicionales, se puede usar un script con `curl` desde el host o un monitor externo para verificar periódicamente la salud.

**Opciones relevantes de health**: La opción principal es `health-enabled` (CLI `--health-enabled`, env `KC_HEALTH_ENABLED`), que por defecto está en `false`. No hay muchos parámetros adicionales más allá de eso, pero es importante notar que los checks de salud de Keycloak incluyen por ahora la comprobación de base de datos (y potencialmente otros en el futuro). Si no se desea exponer estos endpoints en la interfaz de administración, existe un modo _legacy_ que los expondría en la interfaz pública (no recomendado por seguridad)​.

**2.3 Habilitar trazas distribuidas (Tracing)**

Además de métricas y health checks, Keycloak soporta _tracing_ distribuido mediante **OpenTelemetry**. Esto nos permite capturar **trazas** – colecciones de _spans_ (segmentos) que describen el recorrido interno de cada petición. Con las trazas podemos, por ejemplo, ver cuánto tiempo tomó la autenticación de un usuario desglosado en: tiempo en Keycloak, tiempo en consultas a la base de datos, llamadas a un proveedor de identidades externo, etc.

Para activar las trazas, se usa la opción `tracing-enabled`. Por ejemplo:

- CLI: `--tracing-enabled=true`
- Env: `KC_TRACING_ENABLED=true`

Al habilitarlo, Keycloak comenzará a exportar datos de traza mediante el protocolo OpenTelemetry (OTel) en formato gRPC hacia un colector. Por defecto, intenta enviarlos a `http://localhost:4317` (puerto estándar de OTLP gRPC). También se establece por defecto un nombre de servicio “`keycloak`” para identificar las trazas de este servicio.

**Configuración de destino de trazas**: Podemos ajustar a dónde envía Keycloak las trazas mediante las siguientes opciones:

- `tracing-endpoint`: URL del colector de OpenTelemetry. Por defecto `http://localhost:4317`. Si, por ejemplo, usamos Jaeger en Docker, podríamos apuntarlo a la dirección del contenedor Jaeger.
- `tracing-protocol`: protocolo para el envío de trazas, puede ser `grpc` (por defecto) u `http/protobuf` según lo que soporte el colector.
- `tracing-service-name`: nombre del servicio a registrar en las trazas (por defecto `"keycloak"`).

Además, es posible configurar **muestreo (sampling)** para no enviar todas las trazas y así reducir la sobrecarga. Por defecto `tracing-sampler-type` es `traceidratio` con `tracing-sampler-ratio=1.0` (es decir, envía el 100% de las trazas). En entornos de producción suele convenir reducir esto (por ejemplo, enviar sólo una fracción de las solicitudes, como 0.1 = 10% de las trazas) para limitar el volumen de datos.

Una vez activado el tracing, necesitarás un sistema para visualizar las trazas. Un ejemplo popular es **Jaeger** (plataforma de tracing open source). Keycloak es compatible con Jaeger a través de OpenTelemetry. Por ejemplo, para pruebas locales se puede ejecutar Jaeger en modo “all-in-one”:

```bash
docker run --name jaeger -p 16686:16686 -p 4317:4317 jaegertracing/all-in-one
```

Esto levanta Jaeger con una interfaz web en el puerto 16686 y un receptor OTLP en el 4317 (donde Keycloak enviará las trazas). Con Keycloak enviando trazas a `localhost:4317`, podrás abrir la UI de Jaeger (http://localhost:16686) y buscar trazas de servicio "keycloak".

## 3. Tipos de métricas disponibles en Keycloak y uso para monitoreo

Al habilitar las métricas, Keycloak expondrá una variedad de métricas que abarcan diferentes aspectos: desde la aplicación en sí (eventos de seguridad, tiempos de respuesta) hasta la infraestructura subyacente (JVM, base de datos, caches). A continuación describimos las principales categorías de métricas disponibles y cómo utilizarlas para monitorear y diagnosticar problemas en Keycloak:

### 3.1 Métricas de eventos de usuario y seguridad

Keycloak puede reportar métricas basadas en los eventos de usuario (esta característica está en vista previa al momento de escribir). Estas métricas cuentan la ocurrencia de distintos tipos de evento, por ejemplo logins exitosos, intentos fallidos, refresco de tokens, registros de usuarios, etc..

- **`keycloak_user_events_total`**: Es un contador que acumula eventos de usuario. Incluye etiquetas (_labels_) que indican el tipo de evento (`event`), el realm (`realm`), el cliente (`client_id`) y el proveedor de identidad externo (`idp`), así como un label `error` que especifica la causa si el evento fue un error. Por ejemplo, `event="login"` junto con `error="invalid_user_credentials"` indicaría un intento de login fallido por credenciales inválidas. Este contador se incrementa cada vez que ocurre un evento.

Uso: Estas métricas son muy útiles para monitorear la actividad de autenticación y detectar problemas de seguridad o de experiencia de usuario. Por ejemplo:

- Un incremento inusual en `keycloak_user_events_total{event="login", error!="")}` (logins fallidos) podría indicar un problema de autenticación o incluso un posible ataque de fuerza bruta.
- Se puede calcular la tasa de éxito/fracaso de logins comparando eventos de `login` con error vs sin error para detectar si los usuarios están teniendo dificultades para autenticarse.
- También sirve para ver la carga de autenticaciones en el tiempo (p. ej. cuántos logins por minuto se están procesando).

Para habilitar estas métricas de eventos, asegúrate de arrancar Keycloak con la característica de vista previa `user-event-metrics` activada y la opción correspondiente: `--features=user-event-metrics --event-metrics-user-enabled=true`. Opcionalmente, puedes limitar qué eventos incluir (con `--event-metrics-user-events=`) o qué etiquetas utilizar (`--event-metrics-user-tags=`), para controlar la cardinalidad de las métricas. De forma predeterminada, se incluyen todos los tipos de evento y únicamente la etiqueta `realm` (las etiquetas `client_id` e `idp` están deshabilitadas por defecto para evitar un cardinalidad muy alta).

Además de eventos de login/logout, Keycloak proporciona otros contadores relacionados con seguridad, como:

- **`keycloak_credentials_password_hashing_validations_total`**: Cuenta las validaciones de contraseña (verificación de hashes) realizadas, con etiquetas como `algorithm` (algoritmo de hashing usado, ej. `bcrypt` o `argon2`), `hashing_strength` (fuerza o factor de costo) y `outcome` (resultado `valid`, `invalid` o `error`). Esta métrica permite monitorear cuántas autenticaciones se están procesando y cuántas fallan por contraseña incorrecta, e incluso ver el costo computacional si usamos algoritmos fuertes.

**Uso para troubleshooting**: Si observamos un número elevado de `invalid` en la métrica de validación de contraseñas, significa que muchas autenticaciones están fallando por contraseñas inválidas – quizás un usuario olvidó su contraseña, o un servicio está intentando autenticarse con credenciales incorrectas. También podríamos detectar si el algoritmo de hashing es demasiado costoso (impactando la CPU) monitoreando la tasa de validaciones y los tiempos de respuesta.

### 3.2 Métricas de solicitudes HTTP (latencia y volumen)

Keycloak expone métricas estándar sobre las peticiones HTTP que procesa. Estas ayudan a monitorear el **rendimiento** (latencia, throughput) y la **carga** del sistema web de Keycloak:

- **`http_server_requests_seconds_count`** y **`http_server_requests_seconds_sum`**: Son contadores que registran, respectivamente, el número de solicitudes HTTP procesadas y la suma de los tiempos que tomó procesarlas. Cada métrica viene etiquetada con información como el método HTTP (`method`), el código de estado de respuesta (`status`), el resultado (`outcome`, por ejemplo `SUCCESS` o `CLIENT_ERROR`/`SERVER_ERROR`), y la _URI_ solicitada (`uri`). Combinando estas dos métricas podemos calcular el tiempo promedio por solicitud, pero más útil es utilizarlas para derivar tasas de RPS (requests per second) y latencias agregadas.
    
- **Histogramas de latencia**: Si activamos los histogramas (`http-metrics-histograms-enabled=true` o mediante SLOs personalizados), también tendremos métricas del tipo `http_server_requests_seconds_bucket{le="X"}` que cuentan cuántas peticiones se completaron en tiempos inferiores a varios umbrales (buckets). Esto permite calcular distribuciones de latencia y percentiles. Por ejemplo, se pueden obtener percentiles como p95 usando la función PromQL `histogram_quantile()` sobre estos buckets. Sin histogramas, aún podemos aproximar la latencia promedio con `http_server_requests_seconds_sum / http_server_requests_seconds_count`, pero los percentiles nos dan mejor idea de colas de rendimiento.
    
- **`http_server_active_requests`**: Gauge (medidor) que indica el número de solicitudes activas _en curso_ en un momento dado. Esto ayuda a ver cuánta concurrencia está atendiendo Keycloak en tiempo real. Un valor consistentemente alto podría indicar que el servidor está constantemente ocupado atendiendo muchas peticiones simultáneas (posible saturación de hilos).
    
- **Métricas de tráfico**: `http_server_bytes_received_sum` / `_count` y `http_server_bytes_written_sum` / `_count` mantienen track del volumen de datos recibidos y enviados por Keycloak. Por ejemplo, `http_server_bytes_written_sum` acumulado junto con `_count` puede dar un tamaño medio de respuesta, o monitorear la cantidad total de datos servidos (útil para detectar, por ej., si se están transfiriendo archivos grandes o muchos tokens JWT voluminosos).
    

**Uso para monitoreo**: Estas métricas permiten crear gráficos de:

- **Throughput**: consultas por segundo (derivando la tasa de incremento de `..._count` por unidad de tiempo).
- **Latencia**: tiempo medio y percentiles. Por ejemplo, podemos monitorear la latencia p95 de las solicitudes de login. Un aumento repentino en latencias puede indicar un problema de desempeño.
- **Errores HTTP**: filtrando por etiquetas, podemos contabilizar cuántas respuestas fueron códigos 5xx (errores de servidor) o 4xx (errores de cliente). En Prometheus, por ejemplo, `sum(rate(http_server_requests_seconds_count{outcome="SERVER_ERROR"}[5m]))` nos daría la tasa de errores de servidor en los últimos 5 minutos. Una tasa creciente de errores de servidor es señal de una posible falla interna en Keycloak o en un servicio dependiente (por ejemplo, base de datos inaccesible causando errores 500 en logins).

Además, podemos dividir por URI para ver qué endpoints de Keycloak están siendo más utilizados. Por ejemplo, URIs que incluyan `/realms/{realm}/protocol/openid-connect/token` nos indican solicitudes de emisión de tokens; `/auth/realms/{realm}/account` la consola de cuenta de usuario, etc. Esto ayuda a entender la carga funcional (qué tipo de operaciones predominan).

**Uso para troubleshooting**: Si notamos latencias altas, podemos combinar con otras métricas: por ejemplo, latencias elevadas junto con `http_server_active_requests` alto sugiere que las peticiones se están congestionando (posiblemente la causa sea lenta respuesta de la base de datos o falta de threads). Un aumento de errores 5xx podría correlacionarse con problemas de base de datos (ver métricas de pool a continuación) u otros componentes.

### 3.3 Métricas de la base de datos (pool de conexiones)

Keycloak utiliza un _pool_ de conexiones a la base de datos (usando Agroal bajo el capó). Las métricas asociadas a la base de datos ayudan a identificar cuellos de botella en la persistencia:

- **`agroal_active_count`**: Número de conexiones activas actualmente en uso (transacciones en curso).
- **`agroal_available_count`**: Número de conexiones ociosas disponibles en el pool.
- **`agroal_awaiting_count`**: Número de hilos en espera de obtener una conexión del pool.

En un pool configurado con tamaño máximo fijo, `active_count` tenderá a subir hasta ese máximo bajo carga. Si vemos que con frecuencia `active_count` alcanza el límite y además `awaiting_count` > 0 (hilos esperando), significa que el pool se está quedando corto: las peticiones están esperando por una conexión libre. Esto típicamente se refleja en aumento de latencia en las operaciones que requieren base de datos.

**Uso para troubleshooting**: Un valor significativo de `agroal_awaiting_count` indica saturación del pool. Antes de simplemente aumentar el tamaño del pool, que podría sobrecargar la base de datos, se deben considerar estrategias:

- _Reducir_ el número de hilos de trabajo HTTP de Keycloak (`--http-pool-max-threads`) para que no se generen más peticiones concurrentes de las que la base de datos puede manejar.
- Revisar el uso de caches (ver métricas de caches más abajo): si la cache de _usuarios_ tiene baja efectividad y está causando demasiadas consultas a base de datos, quizás convenga aumentarla antes que escalar el pool o la base de datos.

Un escenario común: si la **latencia SLO** no se está cumpliendo (Keycloak responde lento), y vemos muchas conexiones en espera y pocas disponibles, podemos concluir que la base de datos es el cuello de botella. La métrica `agroal_active_count` también nos indica cuántas transacciones concurrentes maneja Keycloak; si está siempre al tope, quizá necesitemos una base de datos más potente o optimizar consultas.

### 3.4 Métricas de caches y clustering (Infinispan)

Keycloak internamente utiliza caches (basadas en Infinispan) para entidades como usuarios, sesiones, tokens, etc. Un buen desempeño del sistema depende en gran medida de la eficacia de estas caches. Existen métricas para **caches embebidas** que ayudan a monitorear su estado y eficiencia:

Algunas métricas importantes de cache (prefijadas con `vendor_statistics_`):

- **Tamaño de la cache**:
    - `vendor_statistics_approximate_entries` y `vendor_statistics_approximate_entries_unique` indican el número de entradas en la cache en el nodo local, incluyendo o excluyendo copias de respaldo respectivamente. Estas nos dan una idea del tamaño de la cache. Si el tamaño se acerca a la capacidad configurada, puede que estemos expulsando elementos frecuentemente.
- **Lecturas (hits/misses)**:
    - `vendor_statistics_hit_times_seconds_count` / `sum` cuentan cuántas lecturas dieron _hit_ en cache y la suma de sus tiempos.
    - `vendor_statistics_miss_times_seconds_count` / `sum` cuentan las lecturas que resultaron en _miss_ (dato no estaba en cache, hubo que consultarlo en DB).
    - A partir de estos podemos calcular la tasa de _hit ratio_ de la cache: por ejemplo, hits / (hits + misses). Un hit ratio bajo (por ejemplo 5% como mencionaba la documentación de troubleshooting) implica que apenas 1 de cada 20 consultas encuentra resultados en cache, lo cual genera mucha carga a la base de datos. En tal caso, **mitigación**: aumentar el tamaño de esa cache para mejorar la tasa de aciertos, si la memoria lo permite.
- **Escrituras (stores)**:
    - `vendor_statistics_store_times_seconds_count` / `sum` miden cuántas escrituras/actualizaciones se hicieron en la cache y el tiempo total invertido en ellas. Esto refleja cuánta carga de actualización hay; tiempos altos podrían señalar problemas de replicación en cluster.
- **Remociones**:
    - `vendor_statistics_remove_times_seconds_count` / `sum` para eliminaciones de entradas (indicando si estaban presentes o no).
- **Evicción**:
    - Métricas de evicción que indican cuántos elementos fueron expulsados de la cache por políticas de tamaño.
- **Replicación de datos en cluster**:
    - Si Keycloak está en un cluster (varios nodos en un mismo datacenter), JGroups maneja la replicación. Métricas con prefijo `vendor_jgroups_` monitorean el estado de esta comunicación (tiempos de respuesta de replicación síncrona, bytes enviados/recibidos en la red, tamaño del cluster, eventos de partición de red, retransmisiones de mensajes, etc.). _Nota_: Estas métricas jgroups son principalmente diagnósticas y pueden cambiar entre versiones, por lo que se sugiere no depender de ellas en dashboards permanentes, pero son útiles para investigar problemas de cluster (por ejemplo, detectar si hubo una partición de red que dividió el cluster, o si hay retransmisiones frecuentes de mensajes lo que indica problemas de comunicación).

**Uso para monitoreo**: Con las métricas de cache, podemos:

- Vigilar la **efectividad**: un gráfico de hit ratio por cache (usuarios, realms, sesiones, etc.) nos muestra si las caches están cumpliendo su cometido. Hit ratio alto es bueno; si empieza a bajar, puede significar que la cache se está invalidando con frecuencia o no tiene tamaño suficiente.
- Vigilar la **replicación**: en un cluster de Keycloak, monitorear `vendor_jgroups_stats_sync_requests_seconds` (conteo y sum de tiempos) nos indica si las replicaciones entre nodos se mantienen rápidas (estable). Un incremento en los tiempos de replicación o retransmisiones puede preceder a problemas mayores de cluster.
- **Capacidad**: observar `approximate_entries` vs el tamaño máximo configurado de la cache nos puede alertar si estamos alcanzando el límite y provocando evicciones. Evicciones excesivas pueden reducir significativamente el rendimiento (bajo hit ratio).

**Uso para troubleshooting**: Si los tiempos de respuesta de Keycloak son inconsistentes, y la base de datos no parece saturada, podríamos revisar las caches. Un _hit ratio_ bajo en la cache de usuarios, combinado con muchas lecturas a base de datos, explicaría latencias altas en búsquedas de usuarios. La solución podría ser aumentar la cache o revisar por qué se invalidan entradas (¿tal vez una configuración de tiempo de vida muy corto?). En entornos clusterizados, si un nodo está lento replicando datos (ej. su métrica de `sync_requests_seconds_sum` es mucho mayor que la de otros nodos), ese nodo puede estar sobrecargado o tener problemas de red; podría desencadenar colas de espera en los otros nodos que le envían peticiones.

### 3.5 Métricas de la JVM y sistema (rendimiento de la plataforma)

Además de las métricas específicas de Keycloak, el endpoint expone métricas base de **JVM** y del sistema que nos ayudan a monitorear el uso de recursos:

- **Memoria JVM**: Métricas como `jvm_memory_used_bytes` y `jvm_memory_committed_bytes` para distintas áreas de la memoria (heap, non-heap). Útiles para detectar crecimiento de uso de memoria en el heap que podría indicar fugas (leaks) o simplemente carga creciente. El _committed_ nos dice cuánta memoria ha reservado la JVM y _used_ cuánto está en uso actualmente.
- **Garbage Collector**: Métricas como `jvm_gc_pause_seconds_count`, `jvm_gc_pause_seconds_sum` y `jvm_gc_pause_seconds_max` rastrean la cantidad y duración de las pausas de GC. Si observamos pausas muy largas o muy frecuentes, esto impacta el rendimiento (congelando el procesamiento durante la recolección). `jvm_gc_overhead` indica el porcentaje de tiempo de CPU dedicado al GC – por ejemplo, valores consistentemente por encima de 0.1 (10%) son señal de que la JVM está invirtiendo mucho esfuerzo en GC, posiblemente por una carga de objetos muy alta.
- **Hilos (Threads)**: Puede haber métricas como `jvm_threads_peak` (número máximo de hilos simultáneos) y `jvm_threads_active`. Nos indica la concurrencia máxima alcanzada; combinándolo con `http_server_active_requests` entendemos si Keycloak está usando muchos hilos para otras tareas en background o sólo para atender requests.
- **CPU y sistema**: En entornos orquestados, a veces se expone `container_cpu_usage_seconds_total` (segundos de CPU consumidos por el contenedor). También se puede encontrar `system_load_average_1m` (carga promedio del sistema). Estos valores dan idea del estrés general del host.

**Uso para monitoreo**: Estas métricas sirven para asegurarnos de que **Keycloak tiene recursos suficientes**:

- Un gráfico de uso de heap a lo largo del tiempo puede mostrar un patrón sano (subidas y bajadas regulares gracias al GC) o un crecimiento constante sin bajadas, lo cual sugiere un posible memory leak.
- Monitorear el overhead de GC: si empieza a subir con el tiempo, quizás debamos revisar la configuración de la JVM, o escalar verticalmente (más memoria) u horizontalmente (más instancias con menos carga).
- CPU: si `container_cpu_usage` indica que Keycloak usa continuamente, digamos, >80% de un núcleo, bajo carga, podría ser normal según la carga, pero si se acerca al 100% sostenido, estamos en el límite de capacidad de procesamiento. Se podrían activar más instancias de Keycloak o revisar qué está consumiendo CPU (por ejemplo, un algoritmo de hashing de contraseñas muy costoso como Argon2 con parámetros altos podría ser causante de alto CPU en autenticaciones; en ese caso `keycloak_credentials_password_hashing_validations_total` y el algoritmo tag nos darían contexto).
- Las métricas de threads nos ayudan a ver si se están creando muchos hilos (lo cual puede ocurrir si algo no anda bien, aunque en Keycloak la mayoría de hilos son administrados en pools fijos).

**Uso para troubleshooting**: Un caso práctico: Keycloak empieza a comportarse lento y eventualmente se traba. Revisando métricas, vemos que la memoria usada (`jvm_memory_used_bytes`) sube constantemente y se acerca al límite del heap, mientras que los GC empiezan a ser más frecuentes y `jvm_gc_pause_seconds_max` crece (pausas largas). Esto podría indicar una fuga de memoria o simplemente mucha carga de sesiones sin expirar. La acción sería tomar un _heap dump_ para análisis (fuera del alcance de métricas) o escalar la instancia. Pero las métricas nos dieron la pista temprana de un problema de memoria antes de que ocurra un _OutOfMemoryError_.

En síntesis, las diversas métricas de Keycloak nos permiten monitorear la disponibilidad, rendimiento y uso de recursos. La clave del **troubleshooting** es correlacionar: por ejemplo, si sube la latencia HTTP, ¿subió también la espera de DB o bajó el hit ratio de cache? Si hay errores 500, ¿qué pasa con la conexión a DB o algún servicio externo? Si el servicio se cae, ¿hubo un pico de GC o memory leak previo? Combinando estas señales podemos diagnosticar problemas y ajustar la configuración (tamaño de pool, caches, parámetros de JVM, etc.) para mejorar la salud del sistema.

## 4. Configuración de Prometheus y Grafana para visualizar métricas

Una vez que Keycloak expone métricas y health checks, necesitamos herramientas para recopilarlas, almacenarlas y visualizarlas. Los componentes típicos en la stack de observabilidad son **Prometheus** (sistema de monitoreo y base de datos de series temporales) y **Grafana** (plataforma de visualización de dashboards).

**Prometheus** actuará como _scraper_ de las métricas de Keycloak. Esto significa que periódicamente realizará solicitudes HTTP al endpoint `/metrics` de Keycloak y almacenará los valores de las métricas en su base de datos interna. Prometheus también permite definir reglas de alertas utilizando PromQL (el lenguaje de consultas de Prometheus) para evaluar las métricas en tiempo real y generar alertas si se cumplen ciertas condiciones (veremos ejemplos en la siguiente sección).

Para configurar Prometheus, debemos proporcionar un archivo `prometheus.yml` que indique **qué endpoints scrapear** y con qué frecuencia. Un ejemplo mínimo de configuración para recopilar métricas de Keycloak:

```yaml
global:
  scrape_interval: 15s        # Intervalo global de scrap: cada 15 segundos
scrape_configs:
  - job_name: "keycloak"
    metrics_path: "/metrics"
    scheme: "http"
    static_configs:
      - targets: ["localhost:9000"]
```

Explicación: aquí definimos un _job_ llamado "keycloak". En `targets` listamos la URL del endpoint de métricas de Keycloak. Si Prometheus corre en el mismo host que Keycloak, apuntamos a `localhost:9000` (asumiendo que Keycloak escucha en 9000 para métricas). Si estuviese en otra máquina o contenedor, pondríamos la IP o nombre de host accesible de Keycloak. El `scrape_interval` global de 15s significa que cada 15 segundos Prometheus intentará leer las métricas. Podemos ajustar este intervalo según la necesidad de granularidad y la sobrecarga aceptable (15s es un buen punto de partida).

> **Nota:** Si Keycloak requiere autenticación o TLS para el endpoint de métricas, tendríamos que añadir configuración adicional (por ejemplo, `basic_auth` si estuviera protegido con usuario/contraseña, o certificados). Sin embargo, en Keycloak por defecto el endpoint en la interfaz de administración suele estar sin autenticación pero idealmente no expuesto públicamente, sólo accesible internamente.

Una vez Prometheus esté recopilando datos, podremos hacer consultas. Por ejemplo, ingresando a la consola web de Prometheus (usualmente en `http://localhost:9090`) podemos probar consultas como:

- `keycloak_user_events_total` – para ver los contadores de eventos por realm.
- `rate(http_server_requests_seconds_count[1m])` – para ver la tasa de requests por segundo.
- `histogram_quantile(0.95, sum(rate(http_server_requests_seconds_bucket[5m])) by (le))` – para calcular el percentil 95 de latencia de todas las solicitudes en los últimos 5 minutos.

Estas consultas nos devuelven números o series temporales que podemos inspeccionar rápidamente, pero no son muy amigables visualmente para monitoreo continuo. Ahí es donde entra Grafana.

**Grafana** es una herramienta de visualización que se conecta a Prometheus (u otras fuentes de datos) y permite crear _dashboards_ con gráficas, tablas, alertas visuales, etc. Para usar Grafana con nuestras métricas:

1. **Configurar la fuente de datos Prometheus**: Al entrar a Grafana (usualmente en `http://localhost:3000`, credenciales por defecto admin/admin), añadimos una nueva Data Source de tipo Prometheus, apuntándola a la URL donde corre Prometheus (por ejemplo `http://localhost:9090`). Grafana probará la conexión y la guardará.
    
2. **Importar o crear un dashboard**: Podemos construir nuestros propios paneles agregando gráficos manualmente. Por ejemplo, crear un gráfico de líneas con la consulta `rate(http_server_requests_seconds_count[1m])` para mostrar RPS, otro con `histogram_quantile(0.95, sum(rate(http_server_requests_seconds_bucket{outcome="SUCCESS"}[5m])) by (le))` para p95 de latencia de respuestas exitosas, etc. Sin embargo, dado que Keycloak ya tiene un conjunto conocido de métricas, la comunidad ha creado dashboards pre-hechos.
    

Grafana Labs ofrece **dashboards comunitarios** listos para Keycloak. Por ejemplo, existe un **Keycloak Metrics Dashboard** actualizado para las métricas de Keycloak basado en Quarkus (ID de Grafana: **14390**). Este dashboard incluye visualizaciones para las métricas de JVM, métricas HTTP, métricas de base de datos y eventos, entre otras. Para usarlo:

- En Grafana, ve a **Dashboards -> Import -> Import via grafana.com**, e ingresa el ID `14390`.
- Selecciona la fuente de datos Prometheus que configuraste.
- Grafana importará el panel con múltiples gráficas ya configuradas.

Tendrás un dashboard con secciones de _JVM/Sistema_ (uso de heap, GC, CPU), _Métricas de Negocio_ (logins exitosos vs fallidos, tokens emitidos, etc.), _Rendimiento_ (latencias p95, throughput de solicitudes), _Base de Datos_ (uso de conexiones), _Cache_ (hits/miss) y más. Esto te da una vista completa de la salud de Keycloak de un solo vistazo.

Alternativamente, puedes crear un dashboard básico tú mismo para entender las consultas. Por ejemplo:

- Un panel mostrando **Tasa de logins exitosos vs fallidos**: consulta PromQL para exitosos: `sum(rate(keycloak_user_events_total{event="login", error=""}[1m]))` y para fallidos: `sum(rate(keycloak_user_events_total{event="login", error!=""}[1m]))`. Grafana permite graficar ambas en la misma visualización, diferenciadas por color/leyenda.
- Un panel **Latencia p95 de logins**: si habilitaste histogramas SLO para 250ms, podrías filtrar por URI de login. Por ejemplo, `histogram_quantile(0.95, sum(rate(http_server_requests_seconds_bucket{uri=~".*login.*"}[5m])) by (le))`. Esto daría el 95º percentil de tiempo para endpoints cuyo URI contiene "login". Así monitoreas la experiencia de login específicamente.
- Un panel **Uso de conexiones DB**: con `agroal_active_count` y `agroal_available_count` en modo gauge (último valor) para ver cuántas conexiones están en uso y cuántas libres. Incluso un alerta visual si las libres bajan a cero.

Grafana también soporta crear **alertas** basadas en paneles o queries. Por ejemplo, puedes configurar que si la tasa de errores 5xx supera cierto umbral por 5 minutos, dispare una alerta (correo, Slack, etc.). Hablaremos más de estrategias de alerta en la siguiente sección.

Por último, no olvidemos la **visualización de trazas**: Grafana tiene un plugin/compatibilidad con Jaeger (u otros sistemas de tracing) para mostrar trazas directamente dentro de un dashboard. Una alternativa es usar la interfaz nativa de Jaeger. En cualquier caso, una vez que Keycloak está enviando trazas a Jaeger, podrás buscar por servicio "keycloak" y ver detalles de cada traza.

## 5. Estrategias de monitoreo y alertas basadas en métricas

Tener métricas y dashboards es útil, pero el siguiente paso en observabilidad es definir **estrategias de monitoreo activo y alertamiento**. Esto implica decidir qué métricas indicarán el estado de salud del servicio (los llamados **Indicadores de Nivel de Servicio** o **SLIs**) y qué objetivos deseamos alcanzar (**Objetivos de Nivel de Servicio**, **SLOs**), para en base a ellos configurar alertas que notifiquen al equipo cuando los SLOs estén en riesgo o se violen.

Algunas recomendaciones para Keycloak:

- **Disponibilidad del servicio**: Un SLI básico es el _uptime_ o disponibilidad. Podemos medirlo con la métrica `up` de Prometheus para Keycloak (que indica si el endpoint de métricas respondió). Un valor de `up` = 0 sugiere que la instancia está caída o inaccesible. Una forma más robusta es usar los endpoints de health: por ejemplo, configurar en Prometheus un _blackbox exporter_ o similar que haga request a `/health/ready` y exponga una métrica. De cualquier modo, definamos un SLO, por ejemplo _Keycloak debe estar disponible el 99.9% del tiempo_. Esto permite ~44 minutos de downtime al mes como máximo. Para monitorearlo, podríamos usar una regla en Prometheus que calcule el porcentaje de tiempo con `up=1` en ventanas de 5m, 1h, 24h. Una alerta de alta severidad podría dispararse si la disponibilidad en 5 minutos cae por debajo de 100% (o sea, se detectó una caída inmediata), y quizás otra de menor severidad si la disponibilidad acumulada en las últimas 24h baja de 99.9% (indicando degradación sostenida).
    
- **Tasa de errores**: Es crucial saber si Keycloak está devolviendo errores a los clientes (por ejemplo códigos 5xx en las peticiones OIDC). Un SLO posible: _El porcentaje de respuestas con error de servidor debe ser menor al 1%_. Podemos calcular una métrica de porcentaje de errores con PromQL:
    
```bash
    error_rate = ( sum(rate(http_server_requests_seconds_count{outcome="SERVER_ERROR"}[5m])) 
               / sum(rate(http_server_requests_seconds_count[5m])) ) * 100 

```

    Esto calcularía el porcentaje de requests con error en los últimos 5 minutos. Una alerta podría activarse si `error_rate` > 1% por más de X minutos. Incluso se pueden separar errores de autenticación específicos: por ejemplo, una tasa alta de errores en el endpoint `/token` podría significar que un servicio está usando credenciales incorrectas o hay otro problema específico.
    
- **Rendimiento (latencia)**: Para la experiencia de usuario, la latencia es clave. Podríamos fijar un SLO: _El 95% de las solicitudes de login/token deben completarse en < 250 ms_. Con histogramas de latencia habilitados, esto se puede medir. Por ejemplo, usando los buckets de latencia, una consulta que calcule el porcentaje de peticiones de autenticación por debajo de 0.25s vs el total nos da ese valor. Si notamos que baja de 95%, es señal de degradación. Una alerta podría saltar si durante, digamos, 10 minutos el percentil 95 supera los 250ms consistentemente. En Prometheus, sin histogramas podríamos aproximar monitoreando la media, pero es menos preciso. Otra métrica útil es `http_server_active_requests`: si este valor se mantiene alto y creciendo, significa que las peticiones se están acumulando (posible _bottleneck_). Podríamos alertar si `http_server_active_requests` excede cierto umbral (por ejemplo, más de N concurrentes durante mucho tiempo).
    
- **Recursos y capacidad**: Aunque no afectan directamente a usuarios hasta que se agotan, es bueno tener alertas para recursos. Ejemplos:
    
    - **Conexiones DB agotadas**: Si `agroal_awaiting_count` > 0 durante varios intervalos seguidos, alerta (indica que hilos están esperando por conexiones). Alternativamente, si `agroal_active_count` alcanza el máximo configurado del pool y permanece así un tiempo, es señal de saturación.
    - **Heap JVM casi lleno**: Si `jvm_memory_used_bytes` se aproxima al máximo (por ejemplo > 90% de `jvm_memory_committed_bytes` o si este a su vez llega al max heap), alerta preventiva de posible OOM.
    - **CPU alta**: Si el uso de CPU del proceso Keycloak (podemos inferirlo si una sola instancia, o usar Node Exporter en el host) supera X% durante Y tiempo, alerta. Un Keycloak a CPU 100% probablemente cause latencias y debe escalarse.
    - **Cache insuficiente**: No suele ser de alerta inmediata, pero un hit ratio muy bajo sostenido podría ser digno de notificación para tuning. Por ejemplo, < 20% de hits en cache de sesiones podría justificar revisión (aunque esto es más una alarma informativa que crítica).
- **Eventos de seguridad**: Keycloak no solo es infraestructura, también es seguridad. Podríamos establecer _alertas de seguridad_ basadas en eventos:
    
    - Número muy alto de logins fallidos en poco tiempo (posible ataque de contraseña). Por ejemplo, si en 5 minutos hay > X eventos `error="invalid_user_credentials"`, notificar al equipo de seguridad.
    - Cuentas bloqueadas (si se configura bloqueo por intentos fallidos, eventos de tipo `user_disabled_by_*_lockout` podrían ser monitoreados).
    - Muchos registros de usuario nuevos súbitos (podría indicar abuso si no esperado).
    - Estos no necesariamente caen bajo SLO normal, pero son útiles para monitoreo proactivo.

**Implementación de alertas**: En Prometheus, las reglas de alerta se configuran por separado, por ejemplo en un archivo de reglas YAML que Prometheus carga. Una regla de alerta ejemplo podría ser:

```yaml
groups:
- name: keycloak-alerts
  rules:
  - alert: KeycloakDown
    expr: sum(up{job="keycloak"} == 0) >= 1
    for: 1m
    labels:
      severity: critical
    annotations:
      description: "No scrape data from Keycloak for 1 minute (instance possibly down)."
      summary: "Keycloak instance is DOWN"
  - alert: KeycloakHighErrorRate
    expr: ( sum(rate(http_server_requests_seconds_count{job="keycloak", outcome="SERVER_ERROR"}[5m])) 
            / sum(rate(http_server_requests_seconds_count{job="keycloak"}[5m])) ) > 0.01
    for: 5m
    labels:
      severity: high
    annotations:
      description: "Over 1% of Keycloak requests are errors (5xx) in the last 5m."
      summary: "High error rate in Keycloak"
```

Aquí, `KeycloakDown` se dispara si en el job keycloak `up` es 0 por >1 minuto. Y `KeycloakHighErrorRate` si la proporción de errores de servidor supera 0.01 (1%) por 5 min. Estas alertas luego las enviaría Prometheus a un _Alertmanager_ (no incluido en nuestro compose, pero normalmente configurado) o Grafana podría gestionar la notificación si se usan _Grafana Alerts_.

**SLOs y seguimiento de nivel de servicio**: Más allá de alertas reactivas, se recomienda hacer seguimiento continuo de SLOs y tener reportes mensuales. Por ejemplo, monitorear cuántos 9s de disponibilidad se lograron, cuántas brechas de SLO de latencia ocurrieron. Herramientas como Grafana tienen plugins de SLO o podemos usar grabaciones (recording rules) en Prometheus para calcular indicadores en ventanas de tiempo largas. Por ejemplo, la documentación de Keycloak sugiere calcular la disponibilidad mensual integrando la métrica `up` sobre 30 días.

En síntesis, una estrategia de monitoreo eficaz para Keycloak debe cubrir:

- **Detección inmediata de caídas** (alerta de down).
- **Detección de degradación en tiempo real** (latencia alta, errores altos).
- **Detección de tendencias peligrosas** (recursos al límite, caches ineficientes).
- **Monitoreo de seguridad** (eventos anómalos).
- **Revisión de SLOs** periódica (asegurar que en el período dado se cumplieron las metas de servicio, p. ej. 99.9% uptime, p95<250ms, error<1%).

Muchas de estas métricas e indicadores ya están disponibles gracias a la observabilidad de Keycloak. Solo resta configurar las reglas y notificaciones adecuadas en las herramientas de monitoreo para aprovecharlos.